library(class)
kmax <- 15
ER1 <- rep(0,kmax)
ER2 <- rep(0,kmax)
#
for (i in 1:kmax){
prediction <- knn(train_input, train_input,train_output, k=i)
prediction2 <- knn(train_input, validate_input, train_output, k=i)
# The confusion matrix for training data is:
CM1 <- table(prediction, df_train$spam)
# The training error rate is:
ER1[i] <- (CM1[1,2]+CM1[2,1])/sum(CM1)
# The confusion matrix for validation data is:
CM2 <- table(prediction2, df_validation$spam)
ER2[i] <- (CM2[1,2]+CM2[2,1])/sum(CM2)
}
plot(c(1,kmax),c(0,1),type="n", xlab="k",ylab="Error Rate")
lines(ER1,col="red")
lines(ER2,col="blue")
legend(10, 0.1, c("Training","Validation"),lty=c(3,1), col=c("red","blue"))
z <- which.min(ER2)
cat("Minimum Validation Error k:", z)
for (i in 1:kmax){
prediction <- knn(train_input, train_input,train_output, k=i)
prediction2 <- knn(train_input, validate_input, train_output, k=i)
na.omit(prediction)
na.omit(predicition2)
# The confusion matrix for training data is:
CM1 <- table(prediction, df_train$spam)
# The training error rate is:
ER1[i] <- (CM1[1,2]+CM1[2,1])/sum(CM1)
# The confusion matrix for validation data is:
CM2 <- table(prediction2, df_validation$spam)
ER2[i] <- (CM2[1,2]+CM2[2,1])/sum(CM2)
}
setwd("C:/Users/gupta/DataMiningProject")
data <- read.csv('app_metadata_cleaned_removed_min_downloads_above_5m.csv')
#attach(data)
data$CATEGORY <- as.factor(data$CATEGORY)
data$PRICE <- as.numeric(data$PRICE)
data$CONTENT_RATING <- as.factor(data$CONTENT_RATING)
data$DOWNLOAD_MIN <- as.numeric(data$DOWNLOAD_MIN)
data$DOWNLOAD_MAX <- as.numeric(data$DOWNLOAD_MAX)
data$SIZE_MEGABYTES <- as.numeric(data$SIZE_MEGABYTES)
data$MIN_REQ_ANDROID_FIRST <- as.factor(data$MIN_REQ_ANDROID_FIRST)
data$TOTAL_REVIEWS <- as.numeric(data$TOTAL_REVIEWS)
data$AVERAGE_RATING <- as.numeric(data$AVERAGE_RATING)
data$X5RATING <- as.numeric(data$X5RATING)
data$X4RATING <- as.numeric(data$X4RATING)
data$X3RATING <- as.numeric(data$X3RATING)
data$X2RATING <- as.numeric(data$X2RATING)
data$X1RATING <- as.numeric(data$X1RATING)
data$spam <- as.factor(data$spam)
set.seed(12345)
df <- data.frame(data$CATEGORY,data$PRICE, data$CONTENT_RATING,data$DOWNLOAD_MIN,data$MIN_REQ_ANDROID_FIRST,data$TOTAL_REVIEWS,data$AVERAGE_RATING,data$spam)
df$CATEGORY <- as.factor(data$CATEGORY)
df$PRICE <- as.numeric(data$PRICE)
df$CONTENT_RATING <- as.factor(data$CONTENT_RATING)
df$DOWNLOAD_MIN <- as.numeric(data$DOWNLOAD_MIN)
df$MIN_REQ_ANDROID_FIRST <- as.factor(data$MIN_REQ_ANDROID_FIRST)
df$TOTAL_REVIEWS <- as.numeric(data$TOTAL_REVIEWS)
df$AVERAGE_RATING <- as.numeric(data$AVERAGE_RATING)
df$spam <- as.factor(data$spam)
data <- df
num.vars <- sapply(data, is.numeric)
data[num.vars] <- lapply(data[num.vars], scale)
Data <- data
set.seed(123457)
library(dummies)
Data <- dummy.data.frame(Data, sep = ".", names = c("CATEGORY","CONTENT_RATING","MIN_REQ_ANDROID_FIRST"))
train <- sample(nrow(Data), .7 * nrow(Data))
df_train <- Data[train,]
df_validation <- Data[-train, ]
#Under Sampling Data
#Taking all the observations with dependent variable = 1
train_under <- df_train[df_train$spam==1,]
#Randomly select observations with dependent variable = 0
zero_spam <- df_train[df_train$spam==0,]
set.seed(123457)
rearrangedZero_spams <-  zero_spam[sample(nrow(zero_spam), length(train_under$spam)),]
train_under <- rbind(train_under, rearrangedZero_spams)
train_over <- df_train[df_train$spam==1,]
train_1 <- train_over
for (i in seq(from=1, to=6, by=1)){
train_over <- rbind(train_over, train_1)
}
train_over
train_oversampling <- rbind(df_train, train_over)
train_input <- as.matrix(df_train[,-45])
train_output <- as.vector(df_train[,45])
validate_input <- as.matrix(df_validation[,-45])
################### Sampling #######################
#install.packages('caret', dependencies = TRUE)
library("caret")
library(class)
kmax <- 15
ER1 <- rep(0,kmax)
ER2 <- rep(0,kmax)
for (i in 1:kmax){
prediction <- knn(train_input, train_input,train_output, k=i)
prediction2 <- knn(train_input, validate_input, train_output, k=i)
na.omit(prediction)
na.omit(predicition2)
# The confusion matrix for training data is:
CM1 <- table(prediction, df_train$spam)
# The training error rate is:
ER1[i] <- (CM1[1,2]+CM1[2,1])/sum(CM1)
# The confusion matrix for validation data is:
CM2 <- table(prediction2, df_validation$spam)
ER2[i] <- (CM2[1,2]+CM2[2,1])/sum(CM2)
}
na.omit(prediction)
na.omit(predicition2)
prediction <- knn(train_input, train_input,train_output, k=3)
prediction2 <- knn(train_input, validate_input, train_output, k=3)
library(dummies)
Data <- dummy.data.frame(Data, sep = ".", names = c("CATEGORY","CONTENT_RATING","MIN_REQ_ANDROID_FIRST"))
train <- sample(nrow(Data), .7 * nrow(Data))
df_train <- Data[train,]
df_validation <- Data[-train, ]
#Under Sampling Data
#Taking all the observations with dependent variable = 1
train_under <- df_train[df_train$spam==1,]
#Randomly select observations with dependent variable = 0
zero_spam <- df_train[df_train$spam==0,]
set.seed(123457)
rearrangedZero_spams <-  zero_spam[sample(nrow(zero_spam), length(train_under$spam)),]
train_under <- rbind(train_under, rearrangedZero_spams)
train_over <- df_train[df_train$spam==1,]
train_1 <- train_over
for (i in seq(from=1, to=6, by=1)){
train_over <- rbind(train_over, train_1)
}
train_over
train_oversampling <- rbind(df_train, train_over)
train_input <- as.matrix(df_train[,-45])
train_output <- as.vector(df_train[,45])
validate_input <- as.matrix(df_validation[,-45])
################### Sampling #######################
#install.packages('caret', dependencies = TRUE)
library("caret")
library(class)
kmax <- 15
ER1 <- rep(0,kmax)
ER2 <- rep(0,kmax)
prediction <- knn(train_input, train_input,train_output, k=3)
setwd("C:/Users/gupta/DataMiningProject")
project <- read.csv("app_metadata_cleaned_removed_min_downloads_above_5m.csv")
# creating duplicate copy of project
data <- project
class(project$PRICE)
# removing not required files
data$APP_ID <- NULL
data$APP_NAME <- NULL
data$DOWNLOADS <- NULL
data$CURRENT_VERSION <- NULL
data$LASTUPDATED <- NULL
data$DEVELOPER_SITE <- NULL
data$DEVELOPER_CONTACT <- NULL
data$DEVELOPER_NAME <- NULL
data$MIN_REQUIRED_ANDROID <- NULL
# creating factors of independent variables and changing variables from integer to numeric
data$CATEGORY <- as.factor(data$CATEGORY)
data$PRICE <- as.numeric(data$PRICE)
data$CONTENT_RATING <- as.factor(data$CONTENT_RATING)
data$DOWNLOAD_MIN <- as.numeric(data$DOWNLOAD_MIN)
data$DOWNLOAD_MAX <- as.numeric(data$DOWNLOAD_MAX)
data$SIZE_MEGABYTES <- as.numeric(data$SIZE_MEGABYTES)
data$MIN_REQ_ANDROID_FIRST <- as.factor(data$MIN_REQ_ANDROID_FIRST)
data$TOTAL_REVIEWS <- as.numeric(data$TOTAL_REVIEWS)
data$AVERAGE_RATING <- as.numeric(data$AVERAGE_RATING)
data$X5RATING <- as.numeric(data$X5RATING)
data$X4RATING <- as.numeric(data$X4RATING)
data$X3RATING <- as.numeric(data$X3RATING)
data$X2RATING <- as.numeric(data$X2RATING)
data$X1RATING <- as.numeric(data$X1RATING)
data$spam <- as.factor(data$spam)
#setting seed and dividing the dataset into Training and Test
set.seed(12345)
train <- sample(nrow(data),0.7*nrow(data))
project_training <- data[train,]
project_Validation <- data[-train,]
library(tree)
library(ISLR)
train_under <- project_training[project_training$spam==1,]
#Randomly select observations with dependent variable = 0
zeroObs <- project_training[project_training$spam==0,]
set.seed(123457)
rearrangedZeroObs <-  zeroObs[sample(nrow(zeroObs), length(train_under$spam)),]
#Appending rows of randomly selected 0s in our undersampled data frame
train_under <- rbind(train_under, rearrangedZeroObs)
tree.project <- tree(spam ~ CATEGORY+PRICE+CONTENT_RATING+DOWNLOAD_MAX+DOWNLOAD_MIN+TOTAL_REVIEWS+AVERAGE_RATING+X5RATING+X4RATING+X3RATING+X2RATING+X1RATING,data = train)
set.seed(12345)
train <- sample(nrow(data),0.7*nrow(data))
project_training <- data[train,]
project_Validation <- data[-train,]
library(tree)
library(ISLR)
#Under Sampling Data
#Taking all the observations with dependent variable = 1
train_under <- project_training[project_training$spam==1,]
#Randomly select observations with dependent variable = 0
zeroObs <- project_training[project_training$spam==0,]
set.seed(123457)
rearrangedZeroObs <-  zeroObs[sample(nrow(zeroObs), length(train_under$spam)),]
#Appending rows of randomly selected 0s in our undersampled data frame
train_under <- rbind(train_under, rearrangedZeroObs)
# creating the classification tree
#tree.project <- tree(spam ~ CATEGORY+PRICE+CONTENT_RATING+DOWNLOAD_MAX+DOWNLOAD_MIN+TOTAL_REVIEWS+AVERAGE_RATING+X5RATING+X4RATING+X3RATING+X2RATING+X1RATING,data = train)
tree.project <- tree(spam ~.,data = train)
tree.project <- tree(spam ~., data = train)
tree.project <- tree(spam ~., data = project_training)
summary(tree.project)
plot(tree.project)
text(tree.project, pretty=6)
tree.predict <- predict(tree.project, project_Validation, type = "class")
confmatrix <- table (project_Validation$spam,tree.predict)
confmatrix
tree.project <- tree(spam ~ .,data = project_train)
tree.project <- tree(spam ~ .,data = project_training)
summary(tree.project)
plot(tree.project)
text(tree.project, pretty=6)
plot(tree.project)
text(tree.project, pretty=6)
tree.predict <- predict(tree.project, project_Validation, type = "class")
confmatrix <- table (project_Validation$spam,tree.predict)
confmatrix
accuracy <- (confmatrix[1,1]+confmatrix[2,2])/sum(confmatrix)
accuracy
sensitivity <- (confmatrix[2,2]/confmatrix[2,2]+confmatrix[2,1])
sensitivity
sensitivity <- (confmatrix[2,2]/(confmatrix[2,2]+confmatrix[2,1]))
sensitivity
tree.project <- tree(spam ~ CATEGORY+PRICE+CONTENT_RATING+DOWNLOAD_MAX+DOWNLOAD_MIN+TOTAL_REVIEWS+AVERAGE_RATING+X5RATING+X4RATING+X3RATING+X2RATING+X1RATING,data = train_under)
summary(tree.project)
#plotting the tree
plot(tree.project)
text(tree.project, pretty=6)
# Creating confusion matrix of pridcted values in test data set
tree.predict <- predict(tree.project, project_Validation, type = "class")
confmatrix <- table (project_Validation$spam,tree.predict)
confmatrix
#calculating the accuracy
accuracy <- (confmatrix[1,1]+confmatrix[2,2])/sum(confmatrix)
accuracy
# calculating the sensitivity
sensitivity <- (confmatrix[2,2]/(confmatrix[2,2]+confmatrix[2,1]))
sensitivity
set.seed(123)
cv.credit <- cv.tree(tree.project,FUN =prune.misclass, K=10)
names(cv.credit)
cv.credit
set.seed(123)
cv.credit <- cv.tree(tree.project,FUN =prune.misclass, K=10)
names(cv.credit)
cv.credit
#pruning the tree with best no. of terminal nodes
prune.credit <- prune.misclass(tree.project, best =8)
plot(prune.credit)
text(prune.credit, pretty=0)
summary(prune.credit)
# Creating confusion matrix of pridcted values in test data set with pruned tree
tree.predict1 <- predict(prune.credit, project_Validation, type = "class")
confmatrix1 <- table (tree.predict1, project_Validation$spam)
confmatrix1
#calculating accuracy of pruned tree
accuracy1 <- (confmatrix1[1,1]+confmatrix1[2,2])/sum(confmatrix1)
accuracy1
# calculating the sensitivity of pruned tree
sensitivity1 <- (confmatrix1[2,2]/(confmatrix1[2,2]+confmatrix1[2,1]))
sensitivity1
# creating the classification tree
tree.project <- tree(spam ~ CATEGORY+PRICE+CONTENT_RATING+DOWNLOAD_MAX+DOWNLOAD_MIN+TOTAL_REVIEWS+AVERAGE_RATING+X5RATING+X4RATING+X3RATING+X2RATING+X1RATING,data = train_under)
summary(tree.project)
#plotting the tree
plot(tree.project)
text(tree.project, pretty=6)
# Creating confusion matrix of pridcted values in test data set
tree.predict <- predict(tree.project, project_Validation, type = "class")
confmatrix <- table (project_Validation$spam,tree.predict)
confmatrix
#calculating the accuracy
accuracy <- (confmatrix[1,1]+confmatrix[2,2])/sum(confmatrix)
accuracy
# calculating the sensitivity
sensitivity <- (confmatrix[2,2]/(confmatrix[2,2]+confmatrix[2,1]))
sensitivity
# Cross validation technique to find the best tree
set.seed(123)
cv.credit <- cv.tree(tree.project,FUN =prune.misclass, K=10)
names(cv.credit)
cv.credit
#pruning the tree with best no. of terminal nodes
prune.credit <- prune.misclass(tree.project, best =8)
plot(prune.credit)
text(prune.credit, pretty=0)
summary(prune.credit)
# Creating confusion matrix of pridcted values in test data set with pruned tree
tree.predict1 <- predict(prune.credit, project_Validation, type = "class")
confmatrix1 <- table (tree.predict1, project_Validation$spam)
confmatrix1
#calculating accuracy of pruned tree
accuracy1 <- (confmatrix1[1,1]+confmatrix1[2,2])/sum(confmatrix1)
accuracy1
# calculating the sensitivity of pruned tree
sensitivity1 <- (confmatrix1[2,2]/(confmatrix1[2,2]+confmatrix1[2,1]))
sensitivity1
#oversampling the data
train_over <- project_training[project_training$spam==1,]
train_1 <- train_over
for (i in seq(from=1, to=6, by=1)){
train_over <- rbind(train_over, train_1)
}
train_over
train_oversampling <- rbind(project_training, train_over)
# creating the classification tree
tree.project <- tree(spam ~ CATEGORY+PRICE+CONTENT_RATING+DOWNLOAD_MAX+DOWNLOAD_MIN+TOTAL_REVIEWS+AVERAGE_RATING+X5RATING+X4RATING+X3RATING+X2RATING+X1RATING,data = project_training)
summary(tree.project)
#plotting the tree
plot(tree.project)
text(tree.project, pretty=6)
# Creating confusion matrix of pridcted values in test data set
tree.predict <- predict(tree.project, project_Validation, type = "class")
confmatrix <- table (project_Validation$spam,tree.predict)
confmatrix
#calculating the accuracy
accuracy <- (confmatrix[1,1]+confmatrix[2,2])/sum(confmatrix)
accuracy
# calculating the sensitivity
sensitivity <- (confmatrix[2,2]/(confmatrix[2,2]+confmatrix[2,1]))
sensitivity
# Cross validation technique to find the best tree
set.seed(123)
cv.credit <- cv.tree(tree.project,FUN =prune.misclass, K=10)
names(cv.credit)
cv.credit
#pruning the tree with best no. of terminal nodes
prune.credit <- prune.misclass(tree.project, best =8)
plot(prune.credit)
text(prune.credit, pretty=0)
summary(prune.credit)
# Creating confusion matrix of pridcted values in test data set with pruned tree
tree.predict1 <- predict(prune.credit, project_Validation, type = "class")
confmatrix1 <- table (tree.predict1, project_Validation$spam)
confmatrix1
#calculating accuracy of pruned tree
accuracy1 <- (confmatrix1[1,1]+confmatrix1[2,2])/sum(confmatrix1)
accuracy1
# calculating the sensitivity of pruned tree
sensitivity1 <- (confmatrix1[2,2]/(confmatrix1[2,2]+confmatrix1[2,1]))
sensitivity1
# creating the classification tree
tree.project <- tree(spam ~ CATEGORY+PRICE+CONTENT_RATING+DOWNLOAD_MAX+DOWNLOAD_MIN+TOTAL_REVIEWS+AVERAGE_RATING+X5RATING+X4RATING+X3RATING+X2RATING+X1RATING,data = train_under)
summary(tree.project)
#plotting the tree
plot(tree.project)
text(tree.project, pretty=6)
# Creating confusion matrix of pridcted values in test data set
tree.predict <- predict(tree.project, project_Validation, type = "class")
confmatrix <- table (project_Validation$spam,tree.predict)
confmatrix
#calculating the accuracy
accuracy <- (confmatrix[1,1]+confmatrix[2,2])/sum(confmatrix)
accuracy
# calculating the sensitivity
sensitivity <- (confmatrix[2,2]/(confmatrix[2,2]+confmatrix[2,1]))
sensitivity
# Cross validation technique to find the best tree
set.seed(123)
cv.credit <- cv.tree(tree.project,FUN =prune.misclass, K=10)
names(cv.credit)
cv.credit
#pruning the tree with best no. of terminal nodes
prune.credit <- prune.misclass(tree.project, best =8)
plot(prune.credit)
text(prune.credit, pretty=0)
summary(prune.credit)
# Creating confusion matrix of pridcted values in test data set with pruned tree
tree.predict1 <- predict(prune.credit, project_Validation, type = "class")
confmatrix1 <- table (tree.predict1, project_Validation$spam)
confmatrix1
#calculating accuracy of pruned tree
accuracy1 <- (confmatrix1[1,1]+confmatrix1[2,2])/sum(confmatrix1)
accuracy1
# calculating the sensitivity of pruned tree
sensitivity1 <- (confmatrix1[2,2]/(confmatrix1[2,2]+confmatrix1[2,1]))
sensitivity1
#oversampling the data
train_over <- project_training[project_training$spam==1,]
train_1 <- train_over
for (i in seq(from=1, to=6, by=1)){
train_over <- rbind(train_over, train_1)
}
train_over
train_oversampling <- rbind(project_training, train_over)
# running classification tree on oversampled data
tree.project <- tree(spam ~ CATEGORY+PRICE+CONTENT_RATING+DOWNLOAD_MAX+DOWNLOAD_MIN+TOTAL_REVIEWS+AVERAGE_RATING+X5RATING+X4RATING+X3RATING+X2RATING+X1RATING,data = train_oversampling)
summary(tree.project)
#plotting the tree
plot(tree.project)
text(tree.project, pretty=6)
# Creating confusion matrix
tree.predict <- predict(tree.project, project_Validation, type = "class")
confmatrix <- table (tree.predict, project_Validation$spam)
confmatrix
# calculating accuracy
accuracy <- (confmatrix[1,1]+confmatrix[2,2])/sum(confmatrix)
accuracy
# calculating the sensitivity
sensitivity <- (confmatrix[2,2]/(confmatrix[2,2]+confmatrix[2,1]))
sensitivity
#running cross validation technique to fnd best tree
set.seed(12345)
cv.credit <- cv.tree(tree.project,FUN =prune.misclass, K=10)
names(cv.credit)
cv.credit
#pruning the tree
prune.credit <- prune.misclass(tree.project, best =4)
plot(prune.credit)
text(prune.credit, pretty=0)
summary(prune.credit)
# calcuate confusion matrix
tree.predict1 <- predict(prune.credit, project_Validation, type = "class")
confmatrix1 <- table (tree.predict1, project_Validation$spam)
confmatrix1
# calculating accuracy of pruned tree
accuracy1 <- (confmatrix1[1,1]+confmatrix1[2,2])/sum(confmatrix1)
accuracy1
# calculating the sensitivity of pruned tree
sensitivity <- (confmatrix[2,2]/(confmatrix[2,2]+confmatrix[2,1]))
sensitivity
# creating the classification tree
tree.project <- tree(spam ~ CATEGORY+PRICE+CONTENT_RATING+DOWNLOAD_MAX+DOWNLOAD_MIN+TOTAL_REVIEWS+AVERAGE_RATING+X5RATING+X4RATING+X3RATING+X2RATING+X1RATING,data = train_under)
summary(tree.project)
#plotting the tree
plot(tree.project)
text(tree.project, pretty=6)
# Creating confusion matrix of pridcted values in test data set
tree.predict <- predict(tree.project, project_Validation, type = "class")
confmatrix <- table (project_Validation$spam,tree.predict)
confmatrix
#calculating the accuracy
accuracy <- (confmatrix[1,1]+confmatrix[2,2])/sum(confmatrix)
accuracy
# calculating the sensitivity
sensitivity <- (confmatrix[2,2]/(confmatrix[2,2]+confmatrix[2,1]))
sensitivity
# Cross validation technique to find the best tree
set.seed(123)
cv.credit <- cv.tree(tree.project,FUN =prune.misclass, K=10)
names(cv.credit)
cv.credit
#pruning the tree with best no. of terminal nodes
prune.credit <- prune.misclass(tree.project, best =8)
plot(prune.credit)
text(prune.credit, pretty=0)
summary(prune.credit)
# Creating confusion matrix of pridcted values in test data set with pruned tree
tree.predict1 <- predict(prune.credit, project_Validation, type = "class")
confmatrix1 <- table (tree.predict1, project_Validation$spam)
confmatrix1
#calculating accuracy of pruned tree
accuracy1 <- (confmatrix1[1,1]+confmatrix1[2,2])/sum(confmatrix1)
accuracy1
# calculating the sensitivity of pruned tree
sensitivity1 <- (confmatrix1[2,2]/(confmatrix1[2,2]+confmatrix1[2,1]))
sensitivity1
#oversampling the data
train_over <- project_training[project_training$spam==1,]
train_1 <- train_over
for (i in seq(from=1, to=6, by=1)){
train_over <- rbind(train_over, train_1)
}
train_over
train_oversampling <- rbind(project_training, train_over)
# creating the classification tree
tree.project <- tree(spam ~ CATEGORY+PRICE+CONTENT_RATING+DOWNLOAD_MAX+DOWNLOAD_MIN+TOTAL_REVIEWS+AVERAGE_RATING+X5RATING+X4RATING+X3RATING+X2RATING+X1RATING,data = train_under)
summary(tree.project)
#plotting the tree
plot(tree.project)
text(tree.project, pretty=6)
# Creating confusion matrix of pridcted values in test data set
tree.predict <- predict(tree.project, project_Validation, type = "class")
confmatrix <- table (project_Validation$spam,tree.predict)
confmatrix
#calculating the accuracy
accuracy <- (confmatrix[1,1]+confmatrix[2,2])/sum(confmatrix)
accuracy
# calculating the sensitivity
sensitivity <- (confmatrix[2,2]/(confmatrix[2,2]+confmatrix[2,1]))
sensitivity
# Cross validation technique to find the best tree
set.seed(123)
cv.credit <- cv.tree(tree.project,FUN =prune.misclass, K=10)
names(cv.credit)
cv.credit
#pruning the tree with best no. of terminal nodes
prune.credit <- prune.misclass(tree.project, best =8)
plot(prune.credit)
text(prune.credit, pretty=0)
summary(prune.credit)
# Creating confusion matrix of pridcted values in test data set with pruned tree
tree.predict1 <- predict(prune.credit, project_Validation, type = "class")
confmatrix1 <- table (tree.predict1, project_Validation$spam)
confmatrix1
#calculating accuracy of pruned tree
accuracy1 <- (confmatrix1[1,1]+confmatrix1[2,2])/sum(confmatrix1)
accuracy1
# calculating the sensitivity of pruned tree
sensitivity1 <- (confmatrix1[2,2]/(confmatrix1[2,2]+confmatrix1[2,1]))
sensitivity1
